7/16 5:06妈的失眠了
title.principals.tsv.gz数据集貌似是可以使用的。如果扩展需要可以使用评分信息title.ratings.tsv.gz
imdb现在下载貌似必须注册aws，没有境外信用卡呀 淘宝买了一个但是卖家不发货 暂时找到一个镜像 ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/temporaryaccess/
注意gedit不能解决但是sublime没有问题的编码问题
使用trim.py脚本trim数据，每一行是一个演员。然后使用Proprocess进一步处理，每个演员出演电影使用"|"隔开

建立电影到演员的倒排表之后，共有701579个电影，进一步的，如果一个电影只有一个演员出演，那么可以直接去掉这部电影,处理之后共510396个电影。
同样的，如果演员参演电影为0可以直接去掉，去掉之后共96w左右演员。如果对倒排表再进行一次倒排，则可以去掉所有死链接的节点，以后考虑做。

计算了图的规模，predata表中电影数量/演员数量：4514592/1131471=3.9900200712170264,即每个演员的子节点有4个。
inverted表中演员数量/电影数量:4323409/510396=8.470695303254727，即每个电影的子节点有8.5个。
那么大概推算：每个演员的共同出演电影演员数量的3*8=24约为20。6度总规模是20^6-1=6400w演员数据，64M条演员数据。不过共同数量应该小于20.

根据宽搜需要的数据，整理如下：
map接受(数据源：数据)
alldata:	<name,children>
cachedata:	<name,children,distance,status,parent>

map发送(数据源 <key,value>)
alldata.sourcedata	组装成一条open数据，当做cachedata.open发送
alldata.all			<name,children>
cachedata.open		<child,distance,parent>open 状态变成close状态,parent加上自己
cachedata.all		<name,distance,status,parent> 

reduce接受(数据源 value 操作)
self.alldata	children						可能用于组成新的cachedata
self.cachedata	distance,status,parent			写入已有的cachedata
parent			parent's<distance,parent>		组成新的cachedata,但会一直收到source数据，因此还需要当前状态为unknown，状态变为open

其中source一直发送数据是为了启动搜索过程，source和dest使用参数进行传递。
reduce和driver之间通过参数传递信息，driver发送dest,reduce发送搜索到的路径，""表示空
但是好像reduce和map都无法接受修改config，只能考虑通过写入文件的方式传递结果


7/16 20:38
首先可以去掉status信息，cachedata中，只有opendata保存的数据是<name,children,distance,status,parent>,其余的只保存<name>
重新设计协议
map接受(数据源：数据)
alldata:			<name,children>
cachedata.open:		<name,children distance parent>
cachedata.close:	<name>
map发送(数据源 <key,value>)
alldata.sourcedata	组装成一条open数据，当做cachedata.open发送
alldata.other		<name,children>
cachedata.open		<child,distance parent> parent加上自己
cachedata.all		<name,value="<c>">（指定的一个特殊字符串，这个字符串不会再其他数据中出现）
reduce接受(数据源 value 操作)
self.alldata	children						可能用于组成新的cachedata
self.close		"<c>"							写入已有的cachedata
parent			parent's<distance parent>		组成新的cachedata,但会一直收到source数据，因此还需要当前状态为unknown，状态变为open

首先观察原输出，得到时间消耗(多次)：
12928ms;13257ms;12233ms;13243ms;14317ms;19229ms;31282ms;42274ms;29474ms;33563ms;29392ms;27507ms;
12952ms;13403ms;13278ms;12254ms;12177ms;18221ms;29260ms;42803ms;29289ms;38331ms;29564ms;29464ms;
15022ms;13463ms;11262ms;14198ms;14244ms;19286ms;29267ms;41313ms;32321ms;31518ms;28290ms;29531ms; 
158055ms,279715ms
最后两轮的文件大小是(多次)：
385M,387M
更新之后的输出，时间消耗:
14830ms;13277ms;13274ms;13264ms;19532ms;13247ms;12378ms;13220ms;13259ms;11163ms;17245ms;12256ms; 
15049ms;12283ms;14261ms;13317ms;18303ms;13241ms;13282ms;12241ms;13201ms;12221ms;18234ms;11295ms;
14935ms;13274ms;13246ms;12237ms;19176ms;12322ms;13250ms;11225ms;13325ms;14229ms;18213ms;12161ms; 
109665ms,156648ms
文件大小:69.6M


尝试通过DistribtionCacheFile读取open的数据点，但是必须保证数据点足够小。
如果数据点已经太大了没办法读取，就不再读取了，直接发送所有数据。
第11次迭代需要open数据有200000条,总的数据规模大概
如果预读open数据，重新设计协议
map接受(数据源：数据)
opendata			setup的时候读取文件，每个数据一行，全是open数据的name
alldata:			<name,children>
cachedata.open:		<name,children distance parent>
cachedata.close:	<name>
map发送(数据源 <key,value>)
// alldata.sourcedata		组装成一条open数据，当做cachedata.open发送,不能发送cachedata.all数据，必须发送自己<name,children>
alldata.sourcedata		<name,distance parent>，fake自己是被父节点open的
alldata.other&inopen	<name,children>
cachedata.open			<child,distance parent> parent加上自己
cachedata.all			<name,value="<c>">（指定的一个特殊字符串，这个字符串不会再其他数据中出现）
reduce接受(数据源 value 操作)
self.alldata	children						可能用于组成新的cachedata
self.close		"<c>"							写入已有的cachedata
parent			parent's<distance parent>		组成新的cachedata,需要当前状态不是close。需要把children写入opendata

Read 50380 lines, size: 29870;		8400ms; 
Read 715353 lines, size: 193718;	22702ms; 
Read 2551069 lines, size: 635339;	49713ms; 
depth=6的时候没有错误，但是用时较长21407ms。应该设置成5
depth=5:	7999ms;5532ms;5255ms;6251ms;7273ms; 
depth=13:	
6927ms;5541ms;5306ms;5255ms;6221ms;20204ms;32283ms;39317ms;32427ms;22268ms;21252ms;22171ms;23227ms;
6818ms;4590ms;4372ms;5310ms;8310ms;18246ms;31279ms;41263ms;34266ms;23284ms;19219ms;19219ms;18247ms; 
5865ms;4593ms;4305ms;5314ms;6332ms;19326ms;29247ms;39393ms;27348ms;23226ms;19268ms;20220ms;18182ms; 
5019ms;4484ms;4324ms;4297ms;7315ms;20237ms;29288ms;36336ms;28269ms;21263ms;19265ms;19292ms;18200ms; 
3890ms;2398ms;3343ms;2347ms;5262ms;10154ms;27236ms;29294ms;26274ms;13179ms;14234ms;12245ms;16214ms; 
110198ms,166070ms
3760ms;2627ms;3262ms;2220ms;5376ms;10182ms;25217ms;29215ms;24256ms;13199ms;16324ms;11125ms;16196ms; 
conf.set必须在Job.getInstance之前啊我的天!
原版bfs:
13988ms;12317ms;11270ms;13225ms;11265ms;18285ms;31247ms;40305ms;29505ms;29308ms;28537ms;26629ms;
14900ms;13455ms;12270ms;13232ms;13238ms;18236ms;29252ms;41274ms;32355ms;30426ms;29504ms;30445ms;
15198ms;12413ms;14255ms;11240ms;12316ms;19227ms;29285ms;41323ms;30338ms;31436ms;28593ms;27349ms;
13674ms;11464ms;13253ms;13237ms;11245ms;17217ms;30228ms;41321ms;31310ms;31566ms;28349ms;29276ms;
15026ms;12408ms;13261ms;11199ms;14238ms;17273ms;30217ms;41269ms;32507ms;30325ms;29394ms;30349ms;
13034ms;12349ms;11253ms;12231ms;14283ms;18232ms;32325ms;41270ms;28309ms;29370ms;28437ms;30276ms;
13905ms;11420ms;11252ms;14253ms;12241ms;20355ms;31273ms;43366ms;31338ms;29421ms;28438ms;29352ms;
12715ms;12414ms;13277ms;12254ms;13262ms;18230ms;30237ms;40285ms;32410ms;31345ms;28451ms;28360ms;
13973ms;12425ms;12299ms;13230ms;14295ms;18231ms;31323ms;42328ms;31328ms;30490ms;28369ms;29404ms;
13643ms;13422ms;12250ms;12235ms;13225ms;19236ms;29241ms;42272ms;31307ms;29386ms;29355ms;30330ms;
26413，25097，31412，71963，61377，57919
时间26413，51510，82922，154885，216262，274181
内存 0，609，19866，864854，17192443，77219407，204523315，292710297，364773800，378929396，385971509，387092567

cacheopen:
13067ms;13453ms;12265ms;15243ms;18239ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
12920ms;13237ms;12263ms;13258ms;17240ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
12753ms;13278ms;13213ms;14241ms;18240ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
14939ms;12297ms;13250ms;12449ms;18238ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
14999ms;14263ms;13246ms;12239ms;18236ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
15089ms;12235ms;13262ms;14225ms;18238ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
13973ms;13272ms;13388ms;14354ms;19332ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
14960ms;13315ms;14260ms;14284ms;19243ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
14884ms;13281ms;12384ms;14246ms;18243ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
13040ms;14254ms;13271ms;14272ms;18245ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
单个时间 27350，26961，45573，53831，30522，30480
时间 27350，54311，99884，153715，184237，214717
内存 570,19210,843302,16271193,59677589,570,19210,843302,16271193,59666852,0,0


prefetch:
3878ms;2227ms;3334ms;2224ms;5270ms;11174ms;27224ms;28575ms;25256ms;15291ms;15231ms;12304ms;18176ms;
3783ms;2273ms;3309ms;2334ms;5267ms;12168ms;26256ms;29149ms;25260ms;14207ms;16363ms;12225ms;16230ms;
4006ms;2541ms;3378ms;2327ms;5381ms;11168ms;27226ms;29239ms;26260ms;11204ms;15228ms;12185ms;17209ms;
4969ms;2442ms;3262ms;2223ms;5288ms;10219ms;25225ms;28215ms;28324ms;14213ms;16223ms;12180ms;16221ms;
4965ms;2564ms;3270ms;2225ms;5420ms;11215ms;25220ms;28224ms;24208ms;15294ms;13318ms;12195ms;17214ms;
3824ms;2442ms;3306ms;2219ms;5272ms;10195ms;26218ms;29216ms;26299ms;11206ms;15205ms;12181ms;18215ms;
4004ms;2426ms;3306ms;2221ms;5260ms;11228ms;25225ms;29230ms;24272ms;14297ms;14252ms;12210ms;17220ms;
3908ms;2269ms;3303ms;2262ms;5343ms;11171ms;28227ms;29156ms;27299ms;14249ms;15235ms;11188ms;16187ms;
3917ms;2434ms;3306ms;2220ms;5485ms;11149ms;27267ms;28361ms;23251ms;12210ms;15393ms;13349ms;16258ms;
4979ms;2441ms;3303ms;2224ms;6538ms;11228ms;27224ms;30216ms;26271ms;13249ms;14283ms;12190ms;16212ms; 
单个时间 9935，7699，37622，54628，28615，29134
时间 9935，17634，55256，109884，138499，167633
内存 
53,587,19744,862994,1660094,60268125,134995104,81897583,91142896,31408116,33245732,27195738,27439404



哈哈哈原来cacheopen比较快的原因是cacheopen写错了，也会出现由于空行导致的arrayoutofbound问题！

TODO:比较BFSCacheOpen和PrefetchOpen的log和ls文件，查出来差距在哪里。可能后者的ls文件比较大，最大达到了100M+.或者就是getConfigure比较费时，或者是MultiOut费事
TODO:优化输入，使输入不再那么严格
http://hadooptutorial.info/mapreduce-multiple-outputs-use-case/给出了一些运行时空间参数设置
TOFO:尝试使用combine
